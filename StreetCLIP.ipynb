{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877b32c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import pipeline\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "import torch\n",
    "import os\n",
    "from PIL import Image\n",
    "from sklearn.metrics.pairwise import cosine_similarity \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac1b1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map\n",
    "with open(\"../assignment2/FIR-02/data02/database/database_lite.json\",\"r\") as f:\n",
    "    m_idx = json.load(f)\n",
    "    m_imgs = np.array(m_idx[\"im_paths\"])\n",
    "    m_loc=np.array(m_idx[\"loc\"])\n",
    "\n",
    "# query\n",
    "with open(\"../assignment2/FIR-02/data02/query/query_lite.json\",\"r\") as f:\n",
    "    q_idx=json.load(f)\n",
    "    q_imgs=np.array(q_idx[\"im_paths\"])\n",
    "    q_loc=np.array(q_idx[\"loc\"])\n",
    "    \n",
    "# loading the relevance judgements\n",
    "with h5py.File(\"../assignment2/FIR-02/data02/london_lite_gt.h5\",\"r\") as f:\n",
    "   fovs = f[\"fov\"][:]\n",
    "   sim = f[\"sim\"][:].astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85314ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(new_session=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe1df2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(ranks, pidx, ks):\n",
    "    recall_at_k = np.zeros(len(ks))\n",
    "    for qidx in range(ranks.shape[0]):\n",
    "        for i, k in enumerate(ks):\n",
    "            if np.sum(np.in1d(ranks[qidx,:k], pidx[qidx])) > 0:\n",
    "                recall_at_k[i:] += 1\n",
    "                break\n",
    "\n",
    "    recall_at_k /= ranks.shape[0]\n",
    "    return recall_at_k\n",
    "\n",
    "def apk(pidx, rank, k):\n",
    "    if len(rank)>k:\n",
    "        rank = rank[:k]\n",
    "\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "\n",
    "    for i,p in enumerate(rank):\n",
    "        if p in pidx and p not in rank[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i+1.0)\n",
    "\n",
    "    return score / min(len(pidx), k)\n",
    "\n",
    "def mapk(ranks, pidxs, k):\n",
    "\n",
    "    return np.mean([apk(a,p,k) for a,p in zip(pidxs, ranks)])\n",
    "\n",
    "def mapk_many(ranks, pidxs, ks):\n",
    "    return np.array([mapk(ranks, pidxs, k) for k in ks], dtype=float)\n",
    "\n",
    "def average_precision(relevant, retrieved):\n",
    "   precisions = []\n",
    "   rel = 0\n",
    "   for i in range(0, len(retrieved)):\n",
    "      if retrieved[i] in relevant:\n",
    "         rel += 1\n",
    "         precisions.append(rel/(i+1))\n",
    "   return sum(precisions) / len(relevant)\n",
    "\n",
    "def mean_average_precision(all_relevant, all_retrieved):\n",
    "   total = 0\n",
    "   count = 0\n",
    "   for qid in all_relevant: \n",
    "      total += average_precision(all_relevant[qid], all_retrieved.get(qid, []))\n",
    "      count += 1\n",
    "   return total / count\n",
    "\n",
    "\n",
    "def l2_normalize(x, axis=1, eps=1e-12):\n",
    "   norm = np.linalg.norm(x, axis=axis, keepdims=True)\n",
    "   return x / (norm + eps)\n",
    "\n",
    "def get_relevant_images(gt_similarity_matrix, query_idx):\n",
    "   return np.where(gt_similarity_matrix[query_idx, :] == 1)[0]\n",
    "\n",
    "def get_retrieved_images(feature_matrix, query_idx):\n",
    "   return np.argsort(feature_matrix[query_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001e6bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CLIPModel.from_pretrained(\"geolocal/StreetCLIP\")\n",
    "processor = CLIPProcessor.from_pretrained(\"geolocal/StreetCLIP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ebc173",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed1656f",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_feats = np.zeros((len(m_imgs), 768), dtype=np.float32)\n",
    "for i,img_name in enumerate(m_imgs):\n",
    "   img = plt.imread(os.path.join('../assignment2/FIR-02/data02/', img_name))\n",
    "   img = Image.fromarray(img)\n",
    "   inputs = processor(images=img, return_tensors=\"pt\").to(device)\n",
    "   with torch.inference_mode():\n",
    "      image_features = model.get_image_features(**inputs)\n",
    "   m_feats[i] = image_features.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271c366c",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_feats = np.zeros((len(m_imgs), 768), dtype=np.float32)\n",
    "for i,img_name in enumerate(q_imgs):\n",
    "   img = plt.imread(os.path.join('../assignment2/FIR-02/data02/', img_name))\n",
    "   img = Image.fromarray(img)\n",
    "   inputs = processor(images=img, return_tensors=\"pt\").to(device)\n",
    "   with torch.inference_mode():\n",
    "      image_features = model.get_image_features(**inputs)\n",
    "   q_feats[i] = image_features.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057eaf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "_m_feats = l2_normalize(m_feats, axis=1)\n",
    "_q_feats = l2_normalize(q_feats, axis=1)\n",
    "\n",
    "similarities = cosine_similarity(_q_feats, _m_feats) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ad8663",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rel, all_ret = {},{}\n",
    "for query_idx in range(len(similarities)):\n",
    "   all_rel[query_idx] = get_relevant_images(sim, query_idx)\n",
    "   all_ret[query_idx] = get_retrieved_images(similarities,query_idx )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6920b3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = np.argsort(-similarities, axis=1)  # (Q, G)\n",
    "\n",
    "Q = similarities.shape[0]\n",
    "pidx = [np.array(all_rel[q], dtype=int) for q in range(Q)]\n",
    "\n",
    "ks = [1, 5, 10, 20]\n",
    "rec = recall(ranks, pidx, ks)\n",
    "recall_at_k = recall(ranks, pidx, ks)\n",
    "mAPs = mapk_many(ranks, pidx, ks)\n",
    "\n",
    "print(f\"MAP : {(mean_average_precision(all_rel, all_ret)*100):.2f}\")\n",
    "for k, r, m in zip(ks, rec, mAPs):\n",
    "    print(f\"Recall@{k}: {r*100:.2f}%   mAP@{k}: {m*100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
