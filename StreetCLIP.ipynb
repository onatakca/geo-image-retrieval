{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef69ad4a-9e75-4db6-a6e0-8b7dc3b9060c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877b32c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from transformers import pipeline\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "from transformers import AutoProcessor, AutoModelForZeroShotImageClassification\n",
    "import torch\n",
    "import os\n",
    "from PIL import Image\n",
    "from sklearn.metrics.pairwise import cosine_similarity \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac1b1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map\n",
    "with open(\"../data/database/database_lite.json\",\"r\") as f:\n",
    "    m_idx = json.load(f)\n",
    "    m_imgs = np.array(m_idx[\"im_paths\"])\n",
    "    m_loc=np.array(m_idx[\"loc\"])\n",
    "\n",
    "# query\n",
    "with open(\"../data/query/query_lite.json\",\"r\") as f:\n",
    "    q_idx=json.load(f)\n",
    "    q_imgs=np.array(q_idx[\"im_paths\"])\n",
    "    q_loc=np.array(q_idx[\"loc\"])\n",
    "    \n",
    "# loading the relevance judgements\n",
    "with h5py.File(\"../data/london_lite_gt.h5\",\"r\") as f:\n",
    "   fovs = f[\"fov\"][:]\n",
    "   sim = f[\"sim\"][:].astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85314ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(new_session=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe1df2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(ranks, pidx, ks):\n",
    "    recall_at_k = np.zeros(len(ks))\n",
    "    for qidx in range(ranks.shape[0]):\n",
    "        for i, k in enumerate(ks):\n",
    "            if np.sum(np.in1d(ranks[qidx,:k], pidx[qidx])) > 0:\n",
    "                recall_at_k[i:] += 1\n",
    "                break\n",
    "\n",
    "    recall_at_k /= ranks.shape[0]\n",
    "    return recall_at_k\n",
    "\n",
    "def apk(pidx, rank, k):\n",
    "    if len(rank)>k:\n",
    "        rank = rank[:k]\n",
    "\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "\n",
    "    for i,p in enumerate(rank):\n",
    "        if p in pidx and p not in rank[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i+1.0)\n",
    "\n",
    "    return score / min(len(pidx), k)\n",
    "\n",
    "def mapk(ranks, pidxs, k):\n",
    "\n",
    "    return np.mean([apk(a,p,k) for a,p in zip(pidxs, ranks)])\n",
    "\n",
    "def mapk_many(ranks, pidxs, ks):\n",
    "    return np.array([mapk(ranks, pidxs, k) for k in ks], dtype=float)\n",
    "\n",
    "def average_precision(relevant, retrieved):\n",
    "   precisions = []\n",
    "   rel = 0\n",
    "   for i in range(0, len(retrieved)):\n",
    "      if retrieved[i] in relevant:\n",
    "         rel += 1\n",
    "         precisions.append(rel/(i+1))\n",
    "   return sum(precisions) / len(relevant)\n",
    "\n",
    "def mean_average_precision(all_relevant, all_retrieved):\n",
    "   total = 0\n",
    "   count = 0\n",
    "   for qid in all_relevant: \n",
    "      total += average_precision(all_relevant[qid], all_retrieved.get(qid, []))\n",
    "      count += 1\n",
    "   return total / count\n",
    "\n",
    "\n",
    "def l2_normalize(x, axis=1, eps=1e-12):\n",
    "   norm = np.linalg.norm(x, axis=axis, keepdims=True)\n",
    "   return x / (norm + eps)\n",
    "\n",
    "def get_relevant_images(gt_similarity_matrix, query_idx):\n",
    "   return np.where(gt_similarity_matrix[query_idx, :] == 1)[0]\n",
    "\n",
    "def get_retrieved_images(feature_matrix, query_idx):\n",
    "   return np.argsort(-feature_matrix[query_idx])\n",
    "\n",
    "def save_results_to_csv(model_name, map_value, recall_at_k, mAPs, csv_path=\"./results/feature_extraction_evaluation.csv\"):\n",
    "    results_dict = {\n",
    "        \"models_name\": model_name,\n",
    "        \"MAP\": map_value * 100,\n",
    "        \"Recall@1\": recall_at_k[0] * 100,\n",
    "        \"Recall@5\": recall_at_k[1] * 100,\n",
    "        \"Recall@10\": recall_at_k[2] * 100,\n",
    "        \"Recall@20\": recall_at_k[3] * 100,\n",
    "        \"mAP@1\": mAPs[0] * 100,\n",
    "        \"mAP@5\": mAPs[1] * 100,\n",
    "        \"mAP@10\": mAPs[2] * 100,\n",
    "        \"mAP@20\": mAPs[3] * 100\n",
    "    }\n",
    "\n",
    "    os.makedirs(os.path.dirname(csv_path), exist_ok=True)\n",
    "    \n",
    "    if os.path.exists(csv_path):\n",
    "        df = pd.read_csv(csv_path)\n",
    "        if model_name in df['models_name'].values:\n",
    "            df.loc[df['models_name'] == model_name] = pd.Series(results_dict)\n",
    "        else:\n",
    "            df = pd.concat([df, pd.DataFrame([results_dict])], ignore_index=True)\n",
    "    else:\n",
    "        df = pd.DataFrame([results_dict])\n",
    "    \n",
    "    df.to_csv(csv_path, index=False)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ad8663",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIP_MODELS = {\n",
    "    \"geolocal/StreetCLIP\": 1024,\n",
    "    \"openai/clip-vit-large-patch14\": 1024,\n",
    "    \"openai/clip-vit-base-patch16\": 768,\n",
    "    \"openai/clip-vit-base-patch32\": 768,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597c0027-e1eb-447b-825e-40c13743caf8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for MODEL_TO_USE, feat_dim in CLIP_MODELS.items():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Loading model: {MODEL_TO_USE}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    try:\n",
    "        if MODEL_TO_USE == \"geolocal/StreetCLIP\":\n",
    "            processor = AutoProcessor.from_pretrained(MODEL_TO_USE)\n",
    "            model = AutoModelForZeroShotImageClassification.from_pretrained(MODEL_TO_USE)\n",
    "        else:\n",
    "            model = CLIPModel.from_pretrained(MODEL_TO_USE)\n",
    "            processor = CLIPProcessor.from_pretrained(MODEL_TO_USE)\n",
    "            \n",
    "        device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "        model = model.to(device)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue\n",
    "    if os.path.exists(f\"./results/pooling_comparison/{MODEL_TO_USE.replace('/', '_')}_pooling_comparison.csv\"):\n",
    "        continue\n",
    "\n",
    "    m_feats_cls = np.zeros((len(m_imgs), feat_dim), dtype=np.float32)\n",
    "    m_feats_mean = np.zeros((len(m_imgs), feat_dim), dtype=np.float32)\n",
    "    m_feats_mean_no_cls = np.zeros((len(m_imgs), feat_dim), dtype=np.float32)\n",
    "    m_feats_max = np.zeros((len(m_imgs), feat_dim), dtype=np.float32)\n",
    "    m_feats_gem = np.zeros((len(m_imgs), feat_dim), dtype=np.float32)\n",
    "    \n",
    "    p = 3.0\n",
    "    \n",
    "    for i, img_name in enumerate(tqdm(m_imgs)):\n",
    "        img = Image.open(os.path.join('../data/', img_name))\n",
    "        inputs = processor(images=img, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        with torch.inference_mode():\n",
    "            vision_outputs = model.vision_model(**inputs)\n",
    "        \n",
    "        m_feats_cls[i] = vision_outputs.pooler_output[0].cpu().numpy()\n",
    "        m_feats_mean[i] = vision_outputs.last_hidden_state.mean(dim=1)[0].cpu().numpy()\n",
    "        m_feats_mean_no_cls[i] = vision_outputs.last_hidden_state[:, 1:, :].mean(dim=1)[0].cpu().numpy()\n",
    "        m_feats_max[i] = vision_outputs.last_hidden_state.max(dim=1)[0][0].cpu().numpy()\n",
    "        m_feats_gem[i] = vision_outputs.last_hidden_state[:, 1:, :].clamp(min=1e-6).pow(p).mean(dim=1).pow(1./p)[0].cpu().numpy()\n",
    "    \n",
    "    q_feats_cls = np.zeros((len(q_imgs), feat_dim), dtype=np.float32)\n",
    "    q_feats_mean = np.zeros((len(q_imgs), feat_dim), dtype=np.float32)\n",
    "    q_feats_mean_no_cls = np.zeros((len(q_imgs), feat_dim), dtype=np.float32)\n",
    "    q_feats_max = np.zeros((len(q_imgs), feat_dim), dtype=np.float32)\n",
    "    q_feats_gem = np.zeros((len(q_imgs), feat_dim), dtype=np.float32)\n",
    "    \n",
    "    for i, img_name in enumerate(tqdm(q_imgs)):\n",
    "        img = Image.open(os.path.join('../data/', img_name))\n",
    "        inputs = processor(images=img, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        with torch.inference_mode():\n",
    "            vision_outputs = model.vision_model(**inputs)\n",
    "        \n",
    "        q_feats_cls[i] = vision_outputs.pooler_output[0].cpu().numpy()\n",
    "        q_feats_mean[i] = vision_outputs.last_hidden_state.mean(dim=1)[0].cpu().numpy()\n",
    "        q_feats_mean_no_cls[i] = vision_outputs.last_hidden_state[:, 1:, :].mean(dim=1)[0].cpu().numpy()\n",
    "        q_feats_max[i] = vision_outputs.last_hidden_state.max(dim=1)[0][0].cpu().numpy()\n",
    "        q_feats_gem[i] = vision_outputs.last_hidden_state[:, 1:, :].clamp(min=1e-6).pow(p).mean(dim=1).pow(1./p)[0].cpu().numpy()\n",
    "    \n",
    "    m_feats_cls = l2_normalize(m_feats_cls, axis=1)\n",
    "    m_feats_mean = l2_normalize(m_feats_mean, axis=1)\n",
    "    m_feats_mean_no_cls = l2_normalize(m_feats_mean_no_cls, axis=1)\n",
    "    m_feats_max = l2_normalize(m_feats_max, axis=1)\n",
    "    m_feats_gem = l2_normalize(m_feats_gem, axis=1)\n",
    "    \n",
    "    q_feats_cls = l2_normalize(q_feats_cls, axis=1)\n",
    "    q_feats_mean = l2_normalize(q_feats_mean, axis=1)\n",
    "    q_feats_mean_no_cls = l2_normalize(q_feats_mean_no_cls, axis=1)\n",
    "    q_feats_max = l2_normalize(q_feats_max, axis=1)\n",
    "    q_feats_gem = l2_normalize(q_feats_gem, axis=1)\n",
    "    \n",
    "    pooling_strategies = {\n",
    "        \"CLS_token\": (q_feats_cls, m_feats_cls),\n",
    "        \"Mean_pooling\": (q_feats_mean, m_feats_mean),\n",
    "        \"Mean_no_CLS\": (q_feats_mean_no_cls, m_feats_mean_no_cls),\n",
    "        \"Max_pooling\": (q_feats_max, m_feats_max),\n",
    "        \"GeM_pooling\": (q_feats_gem, m_feats_gem),\n",
    "    }\n",
    "    \n",
    "    all_results = []\n",
    "    best = None\n",
    "    best_map = 0.0\n",
    "    best_recall_at_k = None\n",
    "    best_mAPs = None\n",
    "    \n",
    "    for pooling_name, (q_feats, m_feats) in pooling_strategies.items():\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Evaluating: {pooling_name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        similarities = cosine_similarity(q_feats, m_feats)\n",
    "        \n",
    "        all_rel = {}\n",
    "        all_ret = {}\n",
    "        for query_idx in range(len(similarities)):\n",
    "            all_rel[query_idx] = get_relevant_images(sim, query_idx)\n",
    "            all_ret[query_idx] = get_retrieved_images(similarities, query_idx)\n",
    "\n",
    "        ranks = np.argsort(-similarities, axis=1) \n",
    "        \n",
    "        Q = similarities.shape[0]\n",
    "        pidx = [np.array(all_rel[q], dtype=int) for q in range(Q)]\n",
    "        \n",
    "        ks = [1, 5, 10, 20]\n",
    "        recall_at_k = recall(ranks, pidx, ks)\n",
    "        mAPs = mapk_many(ranks, pidx, ks)\n",
    "        map_value = mean_average_precision(all_rel, all_ret)\n",
    "        \n",
    "        if best is None or recall_at_k[0] > best_recall_at_k[0]:\n",
    "            best_map = map_value\n",
    "            best = pooling_name\n",
    "            best_recall_at_k = recall_at_k\n",
    "            best_mAPs = mAPs\n",
    "        \n",
    "        print(f\"MODEL: {MODEL_TO_USE}\")\n",
    "        print(f\"Pooling: {pooling_name}\")\n",
    "        print(f\"MAP: {map_value*100:.2f}%\")\n",
    "        for k, r, m in zip(ks, recall_at_k, mAPs):\n",
    "            print(f\"Recall@{k}: {r*100:.2f}%   mAP@{k}: {m*100:.2f}%\")\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        result_dict = {\n",
    "            \"models_name\": f\"{MODEL_TO_USE}_{pooling_name}\",\n",
    "            \"MAP\": map_value * 100,\n",
    "            \"Recall@1\": recall_at_k[0] * 100,\n",
    "            \"Recall@5\": recall_at_k[1] * 100,\n",
    "            \"Recall@10\": recall_at_k[2] * 100,\n",
    "            \"Recall@20\": recall_at_k[3] * 100,\n",
    "            \"mAP@1\": mAPs[0] * 100,\n",
    "            \"mAP@5\": mAPs[1] * 100,\n",
    "            \"mAP@10\": mAPs[2] * 100,\n",
    "            \"mAP@20\": mAPs[3] * 100\n",
    "        }\n",
    "        all_results.append(result_dict)\n",
    "    \n",
    "    df_results = pd.DataFrame(all_results)\n",
    "    CSV_PATH = f\"./results/pooling_comparison/{MODEL_TO_USE.replace('/', '_')}_pooling_comparison.csv\"\n",
    "    os.makedirs(os.path.dirname(CSV_PATH), exist_ok=True)\n",
    "    df_results.to_csv(CSV_PATH, index=False)\n",
    "    \n",
    "    df_main = save_results_to_csv(f\"{MODEL_TO_USE}_{best}\", best_map, best_recall_at_k, best_mAPs)\n",
    "    \n",
    "    del model\n",
    "    if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "    \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "44b3e846-3179-4a48-b86b-e300290beeec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== FINAL MODEL COMPARISON (Best pooling for each) ===\n",
      "                                                 models_name       MAP  Recall@1  Recall@20\n",
      "    facebook/dinov3-vith16plus-pretrain-lvd1689m_GeM_pooling 36.461502      44.0       82.4\n",
      "        facebook/dinov3-vitb16-pretrain-lvd1689m_GeM_pooling 34.103124      41.4       85.4\n",
      "        facebook/dinov3-vitl16-pretrain-lvd1689m_Max_pooling 34.050443      40.4       80.2\n",
      "                    openai/clip-vit-base-patch32_GeM_pooling 28.097766      38.8       82.2\n",
      "          facebook/dinov3-vits16-pretrain-lvd1689m_CLS_token 30.767844      38.0       77.6\n",
      "      facebook/dinov3-vits16plus-pretrain-lvd1689m_CLS_token 29.901573      33.6       73.6\n",
      "                   openai/clip-vit-large-patch14_Mean_no_CLS 28.641709      32.6       80.4\n",
      "                             geolocal/StreetCLIP_GeM_pooling 25.486170      32.6       74.6\n",
      "                   openai/clip-vit-base-patch16_Mean_pooling 24.915458      31.0       76.8\n",
      "                            geolocal/StreetCLIP_Mean_pooling 25.725327      30.0       73.8\n",
      " facebook/dinov3-convnext-tiny-pretrain-lvd1689m_Max_pooling 21.360585      26.6       67.2\n",
      " facebook/dinov3-convnext-base-pretrain-lvd1689m_GeM_pooling 18.573780      24.6       55.6\n",
      "facebook/dinov3-convnext-large-pretrain-lvd1689m_Max_pooling 19.291822      24.2       64.2\n",
      "facebook/dinov3-convnext-small-pretrain-lvd1689m_GeM_pooling 17.523884      20.6       60.8\n",
      "         facebook/dinov3-vitl16-pretrain-sat493m_GeM_pooling 13.405288      17.4       58.4\n"
     ]
    }
   ],
   "source": [
    "df_final = pd.read_csv(\"./results/feature_extraction_evaluation.csv\")\n",
    "print(\"\\n=== FINAL MODEL COMPARISON (Best pooling for each) ===\")\n",
    "print(df_final[['models_name', 'MAP', 'Recall@1', 'Recall@20']].sort_values('Recall@1', ascending=False).to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (gemma3)",
   "language": "python",
   "name": "gemma3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
