{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "696d183f-fe15-4a4b-9588-fef079e303a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping import of cpp extensions due to incompatible torch version 2.9.0+cu126 for torchao version 0.13.0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "import torch\n",
    "import os\n",
    "from PIL import Image\n",
    "from sklearn.metrics.pairwise import cosine_similarity \n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8434aa2a-a2a8-48ba-a929-3256bf03d46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map\n",
    "with open(\"../data/database/database_lite.json\",\"r\") as f:\n",
    "    m_idx = json.load(f)\n",
    "    m_imgs = np.array(m_idx[\"im_paths\"])\n",
    "    m_loc=np.array(m_idx[\"loc\"])\n",
    "\n",
    "# query\n",
    "with open(\"../data/query/query_lite.json\",\"r\") as f:\n",
    "    q_idx=json.load(f)\n",
    "    q_imgs=np.array(q_idx[\"im_paths\"])\n",
    "    q_loc=np.array(q_idx[\"loc\"])\n",
    "    \n",
    "# loading the relevance judgements\n",
    "with h5py.File(\"../data/london_lite_gt.h5\",\"r\") as f:\n",
    "   fovs = f[\"fov\"][:]\n",
    "   sim = f[\"sim\"][:].astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "351989c0-5624-4fac-8600-891b85473201",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(new_session=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bfa0d79-f23d-4cc0-8841-1337a25cf13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(ranks, pidx, ks):\n",
    "    recall_at_k = np.zeros(len(ks))\n",
    "    for qidx in range(ranks.shape[0]):\n",
    "        for i, k in enumerate(ks):\n",
    "            if np.sum(np.in1d(ranks[qidx,:k], pidx[qidx])) > 0:\n",
    "                recall_at_k[i:] += 1\n",
    "                break\n",
    "\n",
    "    recall_at_k /= ranks.shape[0]\n",
    "    return recall_at_k\n",
    "\n",
    "def apk(pidx, rank, k):\n",
    "    if len(rank)>k:\n",
    "        rank = rank[:k]\n",
    "\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "\n",
    "    for i,p in enumerate(rank):\n",
    "        if p in pidx and p not in rank[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i+1.0)\n",
    "\n",
    "    return score / min(len(pidx), k)\n",
    "\n",
    "def mapk(ranks, pidxs, k):\n",
    "\n",
    "    return np.mean([apk(a,p,k) for a,p in zip(pidxs, ranks)])\n",
    "\n",
    "def mapk_many(ranks, pidxs, ks):\n",
    "    return np.array([mapk(ranks, pidxs, k) for k in ks], dtype=float)\n",
    "\n",
    "def average_precision(relevant, retrieved):\n",
    "   precisions = []\n",
    "   rel = 0\n",
    "   for i in range(0, len(retrieved)):\n",
    "      if retrieved[i] in relevant:\n",
    "         rel += 1\n",
    "         precisions.append(rel/(i+1))\n",
    "   return sum(precisions) / len(relevant)\n",
    "\n",
    "def mean_average_precision(all_relevant, all_retrieved):\n",
    "   total = 0\n",
    "   count = 0\n",
    "   for qid in all_relevant: \n",
    "      total += average_precision(all_relevant[qid], all_retrieved.get(qid, []))\n",
    "      count += 1\n",
    "   return total / count\n",
    "\n",
    "\n",
    "def l2_normalize(x, axis=1, eps=1e-12):\n",
    "   norm = np.linalg.norm(x, axis=axis, keepdims=True)\n",
    "   return x / (norm + eps)\n",
    "\n",
    "def get_relevant_images(gt_similarity_matrix, query_idx):\n",
    "   return np.where(gt_similarity_matrix[query_idx, :] == 1)[0]\n",
    "\n",
    "def get_retrieved_images(feature_matrix, query_idx):\n",
    "   return np.argsort(-feature_matrix[query_idx])\n",
    "\n",
    "def save_results_to_csv(model_name, map_value, recall_at_k, mAPs, csv_path=\"./results/feature_extraction_evaluation.csv\"):\n",
    "    results_dict = {\n",
    "        \"models_name\": model_name,\n",
    "        \"MAP\": map_value * 100,\n",
    "        \"Recall@1\": recall_at_k[0] * 100,\n",
    "        \"Recall@5\": recall_at_k[1] * 100,\n",
    "        \"Recall@10\": recall_at_k[2] * 100,\n",
    "        \"Recall@20\": recall_at_k[3] * 100,\n",
    "        \"mAP@1\": mAPs[0] * 100,\n",
    "        \"mAP@5\": mAPs[1] * 100,\n",
    "        \"mAP@10\": mAPs[2] * 100,\n",
    "        \"mAP@20\": mAPs[3] * 100\n",
    "    }\n",
    "\n",
    "    os.makedirs(os.path.dirname(csv_path), exist_ok=True)\n",
    "    \n",
    "    if os.path.exists(csv_path):\n",
    "        df = pd.read_csv(csv_path)\n",
    "        if model_name in df['models_name'].values:\n",
    "            df.loc[df['models_name'] == model_name] = pd.Series(results_dict)\n",
    "        else:\n",
    "            df = pd.concat([df, pd.DataFrame([results_dict])], ignore_index=True)\n",
    "    else:\n",
    "        df = pd.DataFrame([results_dict])\n",
    "    \n",
    "    df.to_csv(csv_path, index=False)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "739fa65b-a89a-4e3e-b00c-6d03edfce74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map\n",
    "with open(\"../data/database/database_lite.json\",\"r\") as f:\n",
    "    m_idx = json.load(f)\n",
    "    m_imgs = np.array(m_idx[\"im_paths\"])\n",
    "    m_loc=np.array(m_idx[\"loc\"])\n",
    "\n",
    "# query\n",
    "with open(\"../data/query/query_lite.json\",\"r\") as f:\n",
    "    q_idx=json.load(f)\n",
    "    q_imgs=np.array(q_idx[\"im_paths\"])\n",
    "    q_loc=np.array(q_idx[\"loc\"])\n",
    "    \n",
    "# loading the relevance judgements\n",
    "with h5py.File(\"../data/london_lite_gt.h5\",\"r\") as f:\n",
    "   fovs = f[\"fov\"][:]\n",
    "   sim = f[\"sim\"][:].astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63745088-1a19-494c-8341-929af6c04bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(new_session=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d7229a3-228f-4a98-b621-06f15da62578",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(ranks, pidx, ks):\n",
    "    recall_at_k = np.zeros(len(ks))\n",
    "    for qidx in range(ranks.shape[0]):\n",
    "        for i, k in enumerate(ks):\n",
    "            if np.sum(np.in1d(ranks[qidx,:k], pidx[qidx])) > 0:\n",
    "                recall_at_k[i:] += 1\n",
    "                break\n",
    "\n",
    "    recall_at_k /= ranks.shape[0]\n",
    "    return recall_at_k\n",
    "\n",
    "def apk(pidx, rank, k):\n",
    "    if len(rank)>k:\n",
    "        rank = rank[:k]\n",
    "\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "\n",
    "    for i,p in enumerate(rank):\n",
    "        if p in pidx and p not in rank[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i+1.0)\n",
    "\n",
    "    return score / min(len(pidx), k)\n",
    "\n",
    "def mapk(ranks, pidxs, k):\n",
    "\n",
    "    return np.mean([apk(a,p,k) for a,p in zip(pidxs, ranks)])\n",
    "\n",
    "def mapk_many(ranks, pidxs, ks):\n",
    "    return np.array([mapk(ranks, pidxs, k) for k in ks], dtype=float)\n",
    "\n",
    "def average_precision(relevant, retrieved):\n",
    "   precisions = []\n",
    "   rel = 0\n",
    "   for i in range(0, len(retrieved)):\n",
    "      if retrieved[i] in relevant:\n",
    "         rel += 1\n",
    "         precisions.append(rel/(i+1))\n",
    "   return sum(precisions) / len(relevant)\n",
    "\n",
    "def mean_average_precision(all_relevant, all_retrieved):\n",
    "   total = 0\n",
    "   count = 0\n",
    "   for qid in all_relevant: \n",
    "      total += average_precision(all_relevant[qid], all_retrieved.get(qid, []))\n",
    "      count += 1\n",
    "   return total / count\n",
    "\n",
    "\n",
    "def l2_normalize(x, axis=1, eps=1e-12):\n",
    "   norm = np.linalg.norm(x, axis=axis, keepdims=True)\n",
    "   return x / (norm + eps)\n",
    "\n",
    "def get_relevant_images(gt_similarity_matrix, query_idx):\n",
    "   return np.where(gt_similarity_matrix[query_idx, :] == 1)[0]\n",
    "\n",
    "def get_retrieved_images(feature_matrix, query_idx):\n",
    "   return np.argsort(-feature_matrix[query_idx])\n",
    "\n",
    "def save_results_to_csv(model_name, map_value, recall_at_k, mAPs, csv_path=\"./results/feature_extraction_evaluation.csv\"):\n",
    "    results_dict = {\n",
    "        \"models_name\": model_name,\n",
    "        \"MAP\": map_value * 100,\n",
    "        \"Recall@1\": recall_at_k[0] * 100,\n",
    "        \"Recall@5\": recall_at_k[1] * 100,\n",
    "        \"Recall@10\": recall_at_k[2] * 100,\n",
    "        \"Recall@20\": recall_at_k[3] * 100,\n",
    "        \"mAP@1\": mAPs[0] * 100,\n",
    "        \"mAP@5\": mAPs[1] * 100,\n",
    "        \"mAP@10\": mAPs[2] * 100,\n",
    "        \"mAP@20\": mAPs[3] * 100\n",
    "    }\n",
    "\n",
    "    os.makedirs(os.path.dirname(csv_path), exist_ok=True)\n",
    "    \n",
    "    if os.path.exists(csv_path):\n",
    "        df = pd.read_csv(csv_path)\n",
    "        if model_name in df['models_name'].values:\n",
    "            df.loc[df['models_name'] == model_name] = pd.Series(results_dict)\n",
    "        else:\n",
    "            df = pd.concat([df, pd.DataFrame([results_dict])], ignore_index=True)\n",
    "    else:\n",
    "        df = pd.DataFrame([results_dict])\n",
    "    \n",
    "    df.to_csv(csv_path, index=False)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c9fff23-2fcd-401e-a53b-4bea74d93d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DINOV3_MODELS = {\n",
    "    # ViT models\n",
    "    \"facebook/dinov3-vith16plus-pretrain-lvd1689m\": 1280,\n",
    "}\n",
    "CLIP_MODELS = {\n",
    "    \"openai/clip-vit-base-patch32\": 768,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bebb5a62-0472-42ad-8768-f3cf4a6aa421",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "100%|██████████| 1000/1000 [00:10<00:00, 94.05it/s]\n",
      "100%|██████████| 500/500 [00:05<00:00, 95.84it/s] \n"
     ]
    }
   ],
   "source": [
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "\n",
    "m_feats_gem_clip = np.zeros((len(m_imgs), 768), dtype=np.float32)\n",
    "p = 3.0\n",
    "\n",
    "for i, img_name in enumerate(tqdm(m_imgs)):\n",
    "    img = Image.open(os.path.join('../data/', img_name))\n",
    "    inputs = processor(images=img, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        vision_outputs = model.vision_model(**inputs)\n",
    "\n",
    "    m_feats_gem_clip[i] = vision_outputs.last_hidden_state[:, 1:, :].clamp(min=1e-6).pow(p).mean(dim=1).pow(1./p)[0].cpu().numpy()\n",
    "\n",
    "q_feats_gem_clip = np.zeros((len(q_imgs), 768), dtype=np.float32)\n",
    "\n",
    "for i, img_name in enumerate(tqdm(q_imgs)):\n",
    "    img = Image.open(os.path.join('../data/', img_name))\n",
    "    inputs = processor(images=img, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        vision_outputs = model.vision_model(**inputs)\n",
    "\n",
    "    q_feats_gem_clip[i] = vision_outputs.last_hidden_state[:, 1:, :].clamp(min=1e-6).pow(p).mean(dim=1).pow(1./p)[0].cpu().numpy()\n",
    "\n",
    "m_feats_gem_clip = l2_normalize(m_feats_gem_clip, axis=1)\n",
    "q_feats_gem_clip = l2_normalize(q_feats_gem_clip, axis=1)\n",
    "\n",
    "del model, processor\n",
    "if torch.cuda.is_available(): torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8a879ad-7166-4a47-bb94-4f082a4d765b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:52<00:00, 19.02it/s]\n",
      "100%|██████████| 500/500 [00:26<00:00, 18.55it/s]\n"
     ]
    }
   ],
   "source": [
    "processor = AutoImageProcessor.from_pretrained(\"facebook/dinov3-vith16plus-pretrain-lvd1689m\")\n",
    "model = AutoModel.from_pretrained(\"facebook/dinov3-vith16plus-pretrain-lvd1689m\") \n",
    "device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "\n",
    "m_feats_gem_dinov3 = np.zeros((len(m_imgs), 1280), dtype=np.float32)\n",
    "p = 3.0\n",
    "\n",
    "for i, img_name in enumerate(tqdm(m_imgs)):\n",
    "    img = Image.open(os.path.join('../data/', img_name))\n",
    "    inputs = processor(images=img, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    m_feats_gem_dinov3[i] = outputs.last_hidden_state[:, 1:, :].clamp(min=1e-6).pow(p).mean(dim=1).pow(1./p)[0].cpu().numpy()\n",
    "\n",
    "q_feats_gem_dinov3 = np.zeros((len(q_imgs), 1280), dtype=np.float32)\n",
    "\n",
    "for i, img_name in enumerate(tqdm(q_imgs)):\n",
    "    img = Image.open(os.path.join('../data/', img_name))\n",
    "    inputs = processor(images=img, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    q_feats_gem_dinov3[i] = outputs.last_hidden_state[:, 1:, :].clamp(min=1e-6).pow(p).mean(dim=1).pow(1./p)[0].cpu().numpy()\n",
    "\n",
    "m_feats_gem_dinov3 = l2_normalize(m_feats_gem_dinov3, axis=1)\n",
    "q_feats_gem_dinov3 = l2_normalize(q_feats_gem_dinov3, axis=1)\n",
    "\n",
    "del model, processor\n",
    "if torch.cuda.is_available(): torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57eb5df2-4a0a-49f4-b844-3c5e117189e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_feats_c = np.concatenate([m_feats_gem_clip, m_feats_gem_dinov3], axis=1)\n",
    "q_feats_c = np.concatenate([q_feats_gem_clip, q_feats_gem_dinov3], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c368c648-1447-44fa-9292-0618c097610a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_744/2298934414.py:5: DeprecationWarning: `in1d` is deprecated. Use `np.isin` instead.\n",
      "  if np.sum(np.in1d(ranks[qidx,:k], pidx[qidx])) > 0:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP: 37.45%\n",
      "Recall@1: 44.80%   mAP@1: 44.80%\n",
      "Recall@5: 66.60%   mAP@5: 32.70%\n",
      "Recall@10: 76.00%   mAP@10: 32.23%\n",
      "Recall@20: 85.00%   mAP@20: 34.03%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "similarities = cosine_similarity(q_feats_c, m_feats_c)\n",
    "\n",
    "all_rel = {}\n",
    "all_ret = {}\n",
    "for query_idx in range(len(similarities)):\n",
    "    all_rel[query_idx] = get_relevant_images(sim, query_idx)\n",
    "    all_ret[query_idx] = get_retrieved_images(similarities, query_idx)\n",
    "\n",
    "ranks = np.argsort(-similarities, axis=1) \n",
    "\n",
    "Q = similarities.shape[0]\n",
    "pidx = [np.array(all_rel[q], dtype=int) for q in range(Q)]\n",
    "\n",
    "ks = [1, 5, 10, 20]\n",
    "recall_at_k = recall(ranks, pidx, ks)\n",
    "mAPs = mapk_many(ranks, pidx, ks)\n",
    "map_value = mean_average_precision(all_rel, all_ret)\n",
    "\n",
    "print(f\"MAP: {map_value*100:.2f}%\")\n",
    "for k, r, m in zip(ks, recall_at_k, mAPs):\n",
    "    print(f\"Recall@{k}: {r*100:.2f}%   mAP@{k}: {m*100:.2f}%\")\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169303f4-a47f-4c04-af35-7f65122bf137",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afead28-e0e4-4990-a92c-6057bd3bed98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_TO_USE = \"dinov3-vith16plus-pretrain-lvd1689m+clip-vit-base-patch32\"\n",
    "df_main = save_results_to_csv(f\"{MODEL_TO_USE}_GEM_pool\", map_value, recall_at_k, mAPs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c3a151-045e-4545-a98f-af88e50b5587",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_final = pd.read_csv(\"./results/feature_extraction_evaluation.csv\")\n",
    "print(df_final[['models_name', 'MAP', 'Recall@1', 'Recall@20']].sort_values('Recall@1', ascending=False).to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (gemma3)",
   "language": "python",
   "name": "gemma3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
