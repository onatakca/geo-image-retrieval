{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "281e3cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping import of cpp extensions due to incompatible torch version 2.9.0+cu126 for torchao version 0.13.0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "import torch\n",
    "import os\n",
    "from PIL import Image\n",
    "from sklearn.metrics.pairwise import cosine_similarity \n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5eb6f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map\n",
    "with open(\"../data/database/database_lite.json\",\"r\") as f:\n",
    "    m_idx = json.load(f)\n",
    "    m_imgs = np.array(m_idx[\"im_paths\"])\n",
    "    m_loc=np.array(m_idx[\"loc\"])\n",
    "\n",
    "# query\n",
    "with open(\"../data/query/query_lite.json\",\"r\") as f:\n",
    "    q_idx=json.load(f)\n",
    "    q_imgs=np.array(q_idx[\"im_paths\"])\n",
    "    q_loc=np.array(q_idx[\"loc\"])\n",
    "    \n",
    "# loading the relevance judgements\n",
    "with h5py.File(\"../data/london_lite_gt.h5\",\"r\") as f:\n",
    "   fovs = f[\"fov\"][:]\n",
    "   sim = f[\"sim\"][:].astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f94e74b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(new_session=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87e75352",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(ranks, pidx, ks):\n",
    "    recall_at_k = np.zeros(len(ks))\n",
    "    for qidx in range(ranks.shape[0]):\n",
    "        for i, k in enumerate(ks):\n",
    "            if np.sum(np.in1d(ranks[qidx,:k], pidx[qidx])) > 0:\n",
    "                recall_at_k[i:] += 1\n",
    "                break\n",
    "\n",
    "    recall_at_k /= ranks.shape[0]\n",
    "    return recall_at_k\n",
    "\n",
    "def apk(pidx, rank, k):\n",
    "    if len(rank)>k:\n",
    "        rank = rank[:k]\n",
    "\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "\n",
    "    for i,p in enumerate(rank):\n",
    "        if p in pidx and p not in rank[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i+1.0)\n",
    "\n",
    "    return score / min(len(pidx), k)\n",
    "\n",
    "def mapk(ranks, pidxs, k):\n",
    "\n",
    "    return np.mean([apk(a,p,k) for a,p in zip(pidxs, ranks)])\n",
    "\n",
    "def mapk_many(ranks, pidxs, ks):\n",
    "    return np.array([mapk(ranks, pidxs, k) for k in ks], dtype=float)\n",
    "\n",
    "def average_precision(relevant, retrieved):\n",
    "   precisions = []\n",
    "   rel = 0\n",
    "   for i in range(0, len(retrieved)):\n",
    "      if retrieved[i] in relevant:\n",
    "         rel += 1\n",
    "         precisions.append(rel/(i+1))\n",
    "   return sum(precisions) / len(relevant)\n",
    "\n",
    "def mean_average_precision(all_relevant, all_retrieved):\n",
    "   total = 0\n",
    "   count = 0\n",
    "   for qid in all_relevant: \n",
    "      total += average_precision(all_relevant[qid], all_retrieved.get(qid, []))\n",
    "      count += 1\n",
    "   return total / count\n",
    "\n",
    "\n",
    "def l2_normalize(x, axis=1, eps=1e-12):\n",
    "   norm = np.linalg.norm(x, axis=axis, keepdims=True)\n",
    "   return x / (norm + eps)\n",
    "\n",
    "def get_relevant_images(gt_similarity_matrix, query_idx):\n",
    "   return np.where(gt_similarity_matrix[query_idx, :] == 1)[0]\n",
    "\n",
    "def get_retrieved_images(feature_matrix, query_idx):\n",
    "   return np.argsort(-feature_matrix[query_idx])\n",
    "\n",
    "def save_results_to_csv(model_name, map_value, recall_at_k, mAPs, csv_path=\"./results/feature_extraction_evaluation.csv\"):\n",
    "    results_dict = {\n",
    "        \"models_name\": model_name,\n",
    "        \"MAP\": map_value * 100,\n",
    "        \"Recall@1\": recall_at_k[0] * 100,\n",
    "        \"Recall@5\": recall_at_k[1] * 100,\n",
    "        \"Recall@10\": recall_at_k[2] * 100,\n",
    "        \"Recall@20\": recall_at_k[3] * 100,\n",
    "        \"mAP@1\": mAPs[0] * 100,\n",
    "        \"mAP@5\": mAPs[1] * 100,\n",
    "        \"mAP@10\": mAPs[2] * 100,\n",
    "        \"mAP@20\": mAPs[3] * 100\n",
    "    }\n",
    "\n",
    "    os.makedirs(os.path.dirname(csv_path), exist_ok=True)\n",
    "    \n",
    "    if os.path.exists(csv_path):\n",
    "        df = pd.read_csv(csv_path)\n",
    "        if model_name in df['models_name'].values:\n",
    "            df.loc[df['models_name'] == model_name] = pd.Series(results_dict)\n",
    "        else:\n",
    "            df = pd.concat([df, pd.DataFrame([results_dict])], ignore_index=True)\n",
    "    else:\n",
    "        df = pd.DataFrame([results_dict])\n",
    "    \n",
    "    df.to_csv(csv_path, index=False)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53e3083a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DINOV3_MODELS = {\n",
    "    # ViT models\n",
    "    \"facebook/dinov3-vits16-pretrain-lvd1689m\": 384,\n",
    "    \"facebook/dinov3-vits16plus-pretrain-lvd1689m\": 384,\n",
    "    \"facebook/dinov3-vitb16-pretrain-lvd1689m\": 768,\n",
    "    \"facebook/dinov3-vitl16-pretrain-lvd1689m\": 1024,\n",
    "    \"facebook/dinov3-vith16plus-pretrain-lvd1689m\": 1280,\n",
    "    #\"facebook/dinov3-vit7b16-pretrain-lvd1689m\": 4096,  \n",
    "    \n",
    "    # ConvNeXt models\n",
    "    \"facebook/dinov3-convnext-tiny-pretrain-lvd1689m\": 768,\n",
    "    \"facebook/dinov3-convnext-small-pretrain-lvd1689m\": 768,\n",
    "    \"facebook/dinov3-convnext-base-pretrain-lvd1689m\": 1024,\n",
    "    \"facebook/dinov3-convnext-large-pretrain-lvd1689m\": 1536,\n",
    "    \n",
    "    # SAT pretrained models\n",
    "    \"facebook/dinov3-vitl16-pretrain-sat493m\": 1024,\n",
    "    #\"facebook/dinov3-vit7b16-pretrain-sat493m\": 4096, \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49f45245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Loading model: facebook/dinov3-vits16-pretrain-lvd1689m\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Loading model: facebook/dinov3-vits16plus-pretrain-lvd1689m\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Loading model: facebook/dinov3-vitb16-pretrain-lvd1689m\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Loading model: facebook/dinov3-vitl16-pretrain-lvd1689m\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Loading model: facebook/dinov3-vith16plus-pretrain-lvd1689m\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Loading model: facebook/dinov3-convnext-tiny-pretrain-lvd1689m\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Loading model: facebook/dinov3-convnext-small-pretrain-lvd1689m\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Loading model: facebook/dinov3-convnext-base-pretrain-lvd1689m\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Loading model: facebook/dinov3-convnext-large-pretrain-lvd1689m\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Loading model: facebook/dinov3-vitl16-pretrain-sat493m\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for MODEL_TO_USE, feat_dim in DINOV3_MODELS.items():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Loading model: {MODEL_TO_USE}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    try:\n",
    "        processor = AutoImageProcessor.from_pretrained(MODEL_TO_USE)\n",
    "        model = AutoModel.from_pretrained(MODEL_TO_USE) \n",
    "        device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "        model = model.to(device)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue\n",
    "    if os.path.exists(f\"./results/pooling_comparison/{MODEL_TO_USE.replace('/', '_')}_pooling_comparison.csv\"):\n",
    "        continue\n",
    "    m_feats_cls = np.zeros((len(m_imgs), feat_dim), dtype=np.float32)\n",
    "    m_feats_mean = np.zeros((len(m_imgs), feat_dim), dtype=np.float32)\n",
    "    m_feats_mean_no_cls = np.zeros((len(m_imgs), feat_dim), dtype=np.float32)\n",
    "    m_feats_max = np.zeros((len(m_imgs), feat_dim), dtype=np.float32)\n",
    "    m_feats_gem = np.zeros((len(m_imgs), feat_dim), dtype=np.float32)\n",
    "    \n",
    "    p = 3.0\n",
    "    \n",
    "    for i, img_name in enumerate(tqdm(m_imgs)):\n",
    "        img = Image.open(os.path.join('../data02/', img_name))\n",
    "        inputs = processor(images=img, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        with torch.inference_mode():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        m_feats_cls[i] = outputs.pooler_output[0].cpu().numpy()\n",
    "        m_feats_mean[i] = outputs.last_hidden_state.mean(dim=1)[0].cpu().numpy()\n",
    "        m_feats_mean_no_cls[i] = outputs.last_hidden_state[:, 1:, :].mean(dim=1)[0].cpu().numpy()\n",
    "        m_feats_max[i] = outputs.last_hidden_state.max(dim=1)[0][0].cpu().numpy()\n",
    "        m_feats_gem[i] = outputs.last_hidden_state[:, 1:, :].clamp(min=1e-6).pow(p).mean(dim=1).pow(1./p)[0].cpu().numpy()\n",
    "    \n",
    "    q_feats_cls = np.zeros((len(q_imgs), feat_dim), dtype=np.float32)\n",
    "    q_feats_mean = np.zeros((len(q_imgs), feat_dim), dtype=np.float32)\n",
    "    q_feats_mean_no_cls = np.zeros((len(q_imgs), feat_dim), dtype=np.float32)\n",
    "    q_feats_max = np.zeros((len(q_imgs), feat_dim), dtype=np.float32)\n",
    "    q_feats_gem = np.zeros((len(q_imgs), feat_dim), dtype=np.float32)\n",
    "    \n",
    "    print(\"Extracting query features...\")\n",
    "    for i, img_name in enumerate(tqdm(q_imgs)):\n",
    "        img = Image.open(os.path.join('../data02/', img_name)) \n",
    "        inputs = processor(images=img, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        with torch.inference_mode():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        q_feats_cls[i] = outputs.pooler_output[0].cpu().numpy()\n",
    "        q_feats_mean[i] = outputs.last_hidden_state.mean(dim=1)[0].cpu().numpy()\n",
    "        q_feats_mean_no_cls[i] = outputs.last_hidden_state[:, 1:, :].mean(dim=1)[0].cpu().numpy()\n",
    "        q_feats_max[i] = outputs.last_hidden_state.max(dim=1)[0][0].cpu().numpy()\n",
    "        q_feats_gem[i] = outputs.last_hidden_state[:, 1:, :].clamp(min=1e-6).pow(p).mean(dim=1).pow(1./p)[0].cpu().numpy()\n",
    "    \n",
    "    \n",
    "    m_feats_cls = l2_normalize(m_feats_cls, axis=1)\n",
    "    m_feats_mean = l2_normalize(m_feats_mean, axis=1)\n",
    "    m_feats_mean_no_cls = l2_normalize(m_feats_mean_no_cls, axis=1)\n",
    "    m_feats_max = l2_normalize(m_feats_max, axis=1)\n",
    "    m_feats_gem = l2_normalize(m_feats_gem, axis=1)\n",
    "    \n",
    "    q_feats_cls = l2_normalize(q_feats_cls, axis=1)\n",
    "    q_feats_mean = l2_normalize(q_feats_mean, axis=1)\n",
    "    q_feats_mean_no_cls = l2_normalize(q_feats_mean_no_cls, axis=1)\n",
    "    q_feats_max = l2_normalize(q_feats_max, axis=1)\n",
    "    q_feats_gem = l2_normalize(q_feats_gem, axis=1)\n",
    "    \n",
    "    pooling_strategies = {\n",
    "        \"CLS_token\": (q_feats_cls, m_feats_cls),\n",
    "        \"Mean_pooling\": (q_feats_mean, m_feats_mean),\n",
    "        \"Mean_no_CLS\": (q_feats_mean_no_cls, m_feats_mean_no_cls),\n",
    "        \"Max_pooling\": (q_feats_max, m_feats_max),\n",
    "        \"GeM_pooling\": (q_feats_gem, m_feats_gem),\n",
    "    }\n",
    "    \n",
    "    all_results = []\n",
    "    best = None\n",
    "    best_map = 0.0\n",
    "    best_recall_at_k = None\n",
    "    best_mAPs = None\n",
    "    \n",
    "    for pooling_name, (q_feats, m_feats) in pooling_strategies.items():\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Evaluating: {pooling_name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        similarities = cosine_similarity(q_feats, m_feats)\n",
    "        \n",
    "        ranks = np.argsort(-similarities, axis=1)  # (Q, G)\n",
    "        \n",
    "        Q = similarities.shape[0]\n",
    "        all_rel = {}\n",
    "        pidx = []\n",
    "        for q in range(Q):\n",
    "            all_rel[q] = get_relevant_images(sim, q)\n",
    "            pidx.append(np.array(all_rel[q], dtype=int))\n",
    "        \n",
    "        all_ret = {q: ranks[q] for q in range(Q)}\n",
    "        \n",
    "        ks = [1, 5, 10, 20]\n",
    "        recall_at_k = recall(ranks, pidx, ks)\n",
    "        mAPs = mapk_many(ranks, pidx, ks)\n",
    "        map_value = mean_average_precision(all_rel, all_ret)\n",
    "        \n",
    "        # Track best\n",
    "        if best is None or map_value > best_map:\n",
    "            best_map = map_value\n",
    "            best = pooling_name\n",
    "            best_recall_at_k = recall_at_k\n",
    "            best_mAPs = mAPs\n",
    "        \n",
    "        print(f\"MODEL: {MODEL_TO_USE}\")\n",
    "        print(f\"Pooling: {pooling_name}\")\n",
    "        print(f\"MAP: {map_value*100:.2f}%\")\n",
    "        for k, r, m in zip(ks, recall_at_k, mAPs):\n",
    "            print(f\"Recall@{k}: {r*100:.2f}%   mAP@{k}: {m*100:.2f}%\")\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        result_dict = {\n",
    "            \"models_name\": f\"{MODEL_TO_USE}_{pooling_name}\",\n",
    "            \"pooling_strategy\": pooling_name,\n",
    "            \"MAP\": map_value * 100,\n",
    "            \"Recall@1\": recall_at_k[0] * 100,\n",
    "            \"Recall@5\": recall_at_k[1] * 100,\n",
    "            \"Recall@10\": recall_at_k[2] * 100,\n",
    "            \"Recall@20\": recall_at_k[3] * 100,\n",
    "            \"mAP@1\": mAPs[0] * 100,\n",
    "            \"mAP@5\": mAPs[1] * 100,\n",
    "            \"mAP@10\": mAPs[2] * 100,\n",
    "            \"mAP@20\": mAPs[3] * 100\n",
    "        }\n",
    "        all_results.append(result_dict)\n",
    "    \n",
    "    df_results = pd.DataFrame(all_results)\n",
    "    CSV_PATH = f\"./results/pooling_comparison/{MODEL_TO_USE.replace('/', '_')}_pooling_comparison.csv\"\n",
    "    os.makedirs(os.path.dirname(CSV_PATH), exist_ok=True)\n",
    "    df_results.to_csv(CSV_PATH, index=False)\n",
    "\n",
    "    df_main = save_results_to_csv(f\"{MODEL_TO_USE}_{best}\", best_map, best_recall_at_k, best_mAPs)\n",
    "    \n",
    "    del model\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3699ed25-ea41-4710-9019-d7cb66dac63d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== FINAL MODEL COMPARISON (Best pooling for each) ===\n",
      "                                                 models_name       MAP  Recall@1  Recall@20\n",
      "    facebook/dinov3-vith16plus-pretrain-lvd1689m_GeM_pooling 36.461502      44.0       82.4\n",
      "        facebook/dinov3-vitb16-pretrain-lvd1689m_GeM_pooling 34.103124      41.4       85.4\n",
      "        facebook/dinov3-vitl16-pretrain-lvd1689m_Max_pooling 34.050443      40.4       80.2\n",
      "          facebook/dinov3-vits16-pretrain-lvd1689m_CLS_token 30.767844      38.0       77.6\n",
      "      facebook/dinov3-vits16plus-pretrain-lvd1689m_CLS_token 29.901573      33.6       73.6\n",
      "                   openai/clip-vit-large-patch14_Mean_no_CLS 28.641709      32.6       80.4\n",
      "                    openai/clip-vit-base-patch32_GeM_pooling 28.097766      38.8       82.2\n",
      "                            geolocal/StreetCLIP_Mean_pooling 25.725327      30.0       73.8\n",
      "                   openai/clip-vit-base-patch16_Mean_pooling 24.915458      31.0       76.8\n",
      " facebook/dinov3-convnext-tiny-pretrain-lvd1689m_Max_pooling 21.360585      26.6       67.2\n",
      "facebook/dinov3-convnext-large-pretrain-lvd1689m_Max_pooling 19.291822      24.2       64.2\n",
      " facebook/dinov3-convnext-base-pretrain-lvd1689m_GeM_pooling 18.573780      24.6       55.6\n",
      "facebook/dinov3-convnext-small-pretrain-lvd1689m_GeM_pooling 17.523884      20.6       60.8\n",
      "         facebook/dinov3-vitl16-pretrain-sat493m_GeM_pooling 13.405288      17.4       58.4\n"
     ]
    }
   ],
   "source": [
    "df_final = pd.read_csv(\"./results/feature_extraction_evaluation.csv\")\n",
    "print(\"\\n=== FINAL MODEL COMPARISON (Best pooling for each) ===\")\n",
    "print(df_final[['models_name', 'MAP', 'Recall@1', 'Recall@20']].sort_values('MAP', ascending=False).to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
