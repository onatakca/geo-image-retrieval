{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281e3cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "from transformers import pipeline\n",
    "from transformers.image_utils import load_image\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "import torch\n",
    "import os\n",
    "from PIL import Image\n",
    "from sklearn.metrics.pairwise import cosine_similarity \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5eb6f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map\n",
    "with open(\"../assignment2/FIR-02/data02/database/database_lite.json\",\"r\") as f:\n",
    "    m_idx = json.load(f)\n",
    "    m_imgs = np.array(m_idx[\"im_paths\"])\n",
    "    m_loc=np.array(m_idx[\"loc\"])\n",
    "\n",
    "# query\n",
    "with open(\"../assignment2/FIR-02/data02/query/query_lite.json\",\"r\") as f:\n",
    "    q_idx=json.load(f)\n",
    "    q_imgs=np.array(q_idx[\"im_paths\"])\n",
    "    q_loc=np.array(q_idx[\"loc\"])\n",
    "    \n",
    "# loading the relevance judgements\n",
    "with h5py.File(\"../assignment2/FIR-02/data02/london_lite_gt.h5\",\"r\") as f:\n",
    "   fovs = f[\"fov\"][:]\n",
    "   sim = f[\"sim\"][:].astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94e74b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c020962fea941bbb935c5af92533503",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e75352",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_at_k(all_relevant,all_retrieved, k):\n",
    "   total = 0\n",
    "   count = 0\n",
    "   for qid in all_relevant:\n",
    "      count+= sum(1 for r in all_retrieved[qid][:k] if r in all_relevant[qid]) \n",
    "      total+= len(all_relevant[qid])\n",
    "   \n",
    "   return count / total\n",
    " \n",
    "def average_precision(relevant, retrieved):\n",
    "   precisions = []\n",
    "   rel = 0\n",
    "   for i in range(0, len(retrieved)):\n",
    "      if retrieved[i] in relevant:\n",
    "         rel += 1\n",
    "         precisions.append(rel/(i+1))\n",
    "   return sum(precisions) / len(relevant)\n",
    "\n",
    "def mean_average_precision(all_relevant, all_retrieved):\n",
    "   total = 0\n",
    "   count = 0\n",
    "   for qid in all_relevant: \n",
    "      total += average_precision(all_relevant[qid], all_retrieved.get(qid, []))\n",
    "      count += 1\n",
    "   return total / count\n",
    "\n",
    "def l2_normalize(x, axis=1, eps=1e-12):\n",
    "   norm = np.linalg.norm(x, axis=axis, keepdims=True)\n",
    "   return x / (norm + eps)\n",
    "\n",
    "def get_relevant_images(gt_similarity_matrix, query_idx):\n",
    "   return np.where(gt_similarity_matrix[query_idx, :] == 1)[0]\n",
    "\n",
    "def get_retrieved_images(feature_matrix, query_idx):\n",
    "   return np.argsort(feature_matrix[query_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e3083a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DINO_MODELS = [\n",
    "   \"facebook/dinov3-vits16-pretrain-lvd1689m\",\n",
    "   \"facebook/dinov3-vits16plus-pretrain-lvd1689m\",\n",
    "   \"facebook/dinov3-vitb16-pretrain-lvd1689m\",\n",
    "   \"facebook/dinov3-vitl16-pretrain-lvd1689m\",\n",
    "   \"facebook/dinov3-vith16plus-pretrain-lvd1689m\",\n",
    "   \"facebook/dinov3-vit7b16-pretrain-lvd1689m\",\n",
    "   \"facebook/dinov3-convnext-base-pretrain-lvd1689m\",\n",
    "   \"facebook/dinov3-convnext-large-pretrain-lvd1689m\",\n",
    "   \"facebook/dinov3-convnext-small-pretrain-lvd1689m\",\n",
    "   \"facebook/dinov3-convnext-tiny-pretrain-lvd1689m\",\n",
    "   \"facebook/dinov3-vitl16-pretrain-sat493m\",\n",
    "   \"facebook/dinov3-vit7b16-pretrain-sat493m\"\n",
    "]\n",
    "\n",
    "MODEL_TO_USE = \"facebook/dinov3-convnext-tiny-pretrain-lvd1689m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f45245",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a81283edc09e4ddc8a1e51112e5ba225",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/111M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Marko Haralović\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Marko Haralović\\.cache\\huggingface\\hub\\models--facebook--dinov3-convnext-tiny-pretrain-lvd1689m. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "375c09228a854ad0bdddca47ef85957e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/585 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = pipeline(\n",
    "    model=\"facebook/dinov3-convnext-tiny-pretrain-lvd1689m\",\n",
    "    task=\"image-feature-extraction\", \n",
    ")\n",
    "\n",
    "pretrained_model_name = \"facebook/dinov3-convnext-tiny-pretrain-lvd1689m\"\n",
    "processor = AutoImageProcessor.from_pretrained(pretrained_model_name)\n",
    "model = AutoModel.from_pretrained(\n",
    "    pretrained_model_name, \n",
    "    device_map=\"auto\", \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650e796c",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_feats = np.zeros((len(m_imgs), 768), dtype=np.float32)\n",
    "for i,img_name in enumerate(m_imgs):\n",
    "   img = plt.imread(os.path.join('../assignment2/FIR-02/data02/', img_name))\n",
    "   img = Image.fromarray(img)\n",
    "   # m_feats[i] = np.array(feature_extractor(img)[0])\n",
    "   inputs = processor(images=img, return_tensors=\"pt\").to(model.device)\n",
    "   with torch.inference_mode():\n",
    "      outputs = model(**inputs)\n",
    "   m_feats[i] = outputs.pooler_output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75f27d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_feats = np.zeros((len(m_imgs), 768), dtype=np.float32)\n",
    "for i,img_name in enumerate(q_imgs):\n",
    "   img = plt.imread(os.path.join('../assignment2/FIR-02/data02/', img_name))\n",
    "   img = Image.fromarray(img)\n",
    "   # m_feats[i] = np.array(feature_extractor(img)[0])\n",
    "   inputs = processor(images=img, return_tensors=\"pt\").to(model.device)\n",
    "   with torch.inference_mode():\n",
    "      outputs = model(**inputs)\n",
    "   q_feats[i] = outputs.pooler_output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4f09cae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities = np.zeros((len(q_feats), len(m_feats)))\n",
    "for i, q_feat in enumerate(q_feats):\n",
    "   for j,m_feat in enumerate(m_feats):\n",
    "      similarities[i][j] = cosine_similarity(q_feat,m_feat.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974ea4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rel, all_ret = {},{}\n",
    "for query_idx in range(len(similarities)):\n",
    "   all_rel[query_idx] = get_relevant_images(sim, query_idx)\n",
    "   all_ret[query_idx] = get_retrieved_images(similarities,query_idx )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9c0c1d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "map = mean_average_precision(all_rel, all_ret)\n",
    "recall_at_1 = recall_at_k(all_rel, all_ret, 1)\n",
    "recall_at_5 = recall_at_k(all_rel, all_ret, 5)\n",
    "recall_at_10 = recall_at_k(all_rel, all_ret, 10)\n",
    "recall_at_20 = recall_at_k(all_rel, all_ret, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "eb911e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP : 60.11%\n",
      "recall_at_1 : 0.6666666666666666\n",
      "recall_at_5 : 0.6666666666666666\n",
      "recall_at_10 : 0.6666666666666666\n",
      "recall_at_20 : 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "print(f\"MAP : {map*100:.2f}%\")\n",
    "print(f\"recall_at_1 : {recall_at_1}\")\n",
    "print(f\"recall_at_5 : {recall_at_1}\")\n",
    "print(f\"recall_at_10 : {recall_at_1}\")\n",
    "print(f\"recall_at_20 : {recall_at_1}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
